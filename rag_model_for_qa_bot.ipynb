{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BrCCQLfdVz8IJ4MM-mVafJiBhR14-FjY",
      "authorship_tag": "ABX9TyP9IrrL7zSAkY+kpmP7NeAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1d61b87bbc447ef9ea7727baec4278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_735bf9a2d97140fcb67b8044358a050a",
              "IPY_MODEL_e4d62daefae24810bc28d798a8c97356",
              "IPY_MODEL_9227f2ab876b4202b051794639d9b3ab"
            ],
            "layout": "IPY_MODEL_0ffa9ee5aa984a369197e21332137f99"
          }
        },
        "735bf9a2d97140fcb67b8044358a050a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c247eafd7a554f5582b240c1cec36b80",
            "placeholder": "​",
            "style": "IPY_MODEL_66011cdfc59e4d28973e2cf1ddb987b3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e4d62daefae24810bc28d798a8c97356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb34748ee089477fb353ca0d3c813acc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5757c660ffcf46e3874819d908e48a3b",
            "value": 2
          }
        },
        "9227f2ab876b4202b051794639d9b3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ead45cdf967434d9294e633606960d5",
            "placeholder": "​",
            "style": "IPY_MODEL_6423033887d546bdb207aa5ed79076bf",
            "value": " 2/2 [00:01&lt;00:00,  1.57it/s]"
          }
        },
        "0ffa9ee5aa984a369197e21332137f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c247eafd7a554f5582b240c1cec36b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66011cdfc59e4d28973e2cf1ddb987b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb34748ee089477fb353ca0d3c813acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5757c660ffcf46e3874819d908e48a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ead45cdf967434d9294e633606960d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6423033887d546bdb207aa5ed79076bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/Himanshusabale/b8f31bcd58e702b2e75c6bdf4a5d239b/rag_model_for_qa_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "c1d61b87bbc447ef9ea7727baec4278d",
            "735bf9a2d97140fcb67b8044358a050a",
            "e4d62daefae24810bc28d798a8c97356",
            "9227f2ab876b4202b051794639d9b3ab",
            "0ffa9ee5aa984a369197e21332137f99",
            "c247eafd7a554f5582b240c1cec36b80",
            "66011cdfc59e4d28973e2cf1ddb987b3",
            "cb34748ee089477fb353ca0d3c813acc",
            "5757c660ffcf46e3874819d908e48a3b",
            "4ead45cdf967434d9294e633606960d5",
            "6423033887d546bdb207aa5ed79076bf"
          ]
        },
        "id": "KdxjW4FFJQTl",
        "outputId": "713b43c1-7b75-4f75-dac0-1e298de178ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1d61b87bbc447ef9ea7727baec4278d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "<ipython-input-51-58262bbac2c7>:161: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = llm(prompt)  # Call the LLM directly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What services does your company offer?\n",
            "Answer: consulting, development, and implementation services\n",
            "--------------------\n",
            "Question: What is your area of expertise?\n",
            "Answer: (ii).\n",
            "--------------------\n",
            "Question: How can I contact you?\n",
            "Answer: [ii]\n",
            "--------------------\n",
            "Question: What kind of team do you have?\n",
            "Answer: (ii).\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Imports & Setup\n",
        "import os\n",
        "import pinecone\n",
        "# Updated imports\n",
        "from langchain_pinecone import Pinecone\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "import time\n",
        "import backoff\n",
        "import tiktoken\n",
        "import openai\n",
        "import json\n",
        "from functools import lru_cache\n",
        "import random\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_ENVIRONMENT\"] = \\"\n",
        "\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone_client = pinecone.Pinecone(\n",
        "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
        "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        ")\n",
        "index_name = \"business-qa-bot\"\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "\n",
        "\n",
        "# Sample business data\n",
        "business_data = [\n",
        "    \"Our company specializes in AI-powered solutions for businesses.\",\n",
        "    \"We offer consulting, development, and implementation services.\",\n",
        "    \"Our expertise includes natural language processing, computer vision, and machine learning.\",\n",
        "    \"We have a team of experienced data scientists and engineers.\",\n",
        "    \"We are committed to delivering innovative and effective solutions to our clients.\",\n",
        "    \"Contact us at info@example.com or visit our website at www.example.com.\"\n",
        "]\n",
        "\n",
        "# --- Persistent Caching with Invalidation ---\n",
        "cache_file = 'embeddings_cache.json'\n",
        "cache_expiration_time = 60 * 60 * 24  # Cache expires after 24 hours\n",
        "\n",
        "\n",
        "# Load from cache if available and not expired\n",
        "if os.path.exists(cache_file):\n",
        "    cache_modified_time = os.path.getmtime(cache_file)\n",
        "    if time.time() - cache_modified_time < cache_expiration_time:\n",
        "        with open(cache_file, 'r') as f:\n",
        "            embedding_cache = json.load(f)\n",
        "    else:\n",
        "        # Cache expired, clear it\n",
        "        embedding_cache = {}\n",
        "        print(\"Cache expired. Clearing...\")  # Optional: Print a message\n",
        "else:\n",
        "    embedding_cache = {}\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_embedding(text):\n",
        "    if text in embedding_cache:\n",
        "        return embedding_cache[text]\n",
        "\n",
        "\n",
        " # (For potential use with OpenAI later)\n",
        "    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=5)\n",
        "    def embed_with_backoff():\n",
        "        return embeddings.embed_query(text) #if using OpenAI\n",
        "\n",
        "    # Choose the appropriate embedding method\n",
        "    # embedding = embed_with_backoff()  # Use this if using OpenAI\n",
        "    embedding = embeddings.embed_query(\n",
        "        text\n",
        "    )  # use this if using SentenceTransformerEmbeddings without OpenAI\n",
        "    embedding_cache[text] = embedding  # Store in cache\n",
        "\n",
        "    # Save to cache file\n",
        "    with open(cache_file, \"w\") as f:\n",
        "        json.dump(embedding_cache, f)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize SentenceTransformer embeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone_client = pinecone.Pinecone(\n",
        "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
        "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Check if the index is empty and populate it if necessary\n",
        "if index.describe_index_stats()[\"total_vector_count\"] == 0:\n",
        "    batch_size = 32\n",
        "    encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
        "    for i in range(0, len(business_data), batch_size):\n",
        "        i_end = min(i + batch_size, len(business_data))\n",
        "        batch = business_data[i:i_end]\n",
        "        ids = [str(x) for x in range(i, i_end)]\n",
        "        embeds = [get_embedding(text) for text in batch]\n",
        "        metadata = [{\"text\": text} for text in batch]\n",
        "        vectors = zip(ids, embeds, metadata)\n",
        "        index.upsert(vectors=vectors)\n",
        "\n",
        "# --- Langchain setup and queries ---\n",
        "vectorstore = Pinecone(index, embeddings, \"text\")\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"  # Use \"query\" here\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"query\"], template=template  # Use \"query\" here\n",
        ")\n",
        "\n",
        "# Create LangChain pipeline\n",
        "llm_pipeline = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-xl\",\n",
        "    task=\"text2text-generation\",\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm_pipeline, prompt=prompt_template)\n",
        "\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm_chain, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "# Example queries\n",
        "queries = [\n",
        "    \"What services does your company offer?\",\n",
        "    \"What is your area of expertise?\",\n",
        "    \"How can I contact you?\",\n",
        "    \"What kind of team do you have?\",\n",
        "]\n",
        "llm = llm_pipeline # You already have this defined\n",
        "\n",
        "# Update this part:\n",
        "def generate_answer(query):\n",
        "    \"\"\"Generates an answer to a query using the business_data and LLM.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query)\n",
        "    context = docs[0].page_content\n",
        "    prompt = prompt_template.format(query=query, context=context)  # Format prompt with query and context\n",
        "    answer = llm(prompt)  # Call the LLM directly\n",
        "    return answer\n",
        "\n",
        "for query in queries:\n",
        "    answer = generate_answer(query)\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TyDJsB9484BT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtmB4KEWJRpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
